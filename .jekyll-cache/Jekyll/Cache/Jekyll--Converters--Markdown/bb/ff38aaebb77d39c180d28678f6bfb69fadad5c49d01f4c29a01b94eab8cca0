I"ï<p>This simple code will help you when you canâ€™t use pulic web scanner. :)</p>

<p>í¼ë¸”ë¦­íˆ´ì„ ì“°ê¸° ì–´ë µê±°ë‚˜ ë¶ˆê°€ëŠ¥í•œ ì¥ì†Œì—ì„œ ìš”ê¸´í•˜ê²Œ ì‚¬ìš©í•  ê°„ë‹¨í•œ ìŠ¤ìºë„ˆì´ë‹¤.</p>

<p>feature.</p>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Web Directory / url scan base on %Dic.txt</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />result will be : URL / Response code / Response size</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Simple Subdomain scan on hackertagrget.com</li>
</ul>

<p>Usage : python3 scanner.py â€“help<br />
python3 scanner.py d â€“dir URL.txt (http/https)://target.com</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import requests
import argparse,textwrap
import time
from urllib.request import urlopen
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import re
import ssl

requests.packages.urllib3.disable_warnings()

#ì¸ìê°’ ë°›ì€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
parser = argparse.ArgumentParser(description='scanner test',usage = 'use "python scanner.py --help"',formatter_class=argparse.RawTextHelpFormatter)

#ì¸ìê°’ ë“±ë¡D
parser.add_argument('option', choices=['d','i','s'], default ='d',
                    help= textwrap.dedent('''
                    d = ë””ë ‰í† ë¦¬ ìŠ¤ìº”, text íŒŒì¼ í•„ìš” ex)scanner.py -d --dir url.txt https://target.com
                    i = URL ë‚´ ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ í™•ì¸
                    s = ì„œë¸Œ ë„ë©”ì¸ ìŠ¤ìº” ex)scanner.py -s (http/https)target.com
                    '''))
parser.add_argument("url", help="url(with http/https)")
parser.add_argument("--dir", help='directory',required=False)

#ì¸ìê°’ ì €ì¥
args = parser.parse_args()


#url validationí™•ì¸
def urlcheck(url):
    if not (url.startswith('http://') or url.startswith('https://')):
        print('you need define http/https to your target, defualt = http')
        url = 'http://'+url
    return url

# ë””ë ‰í† ë¦¬ ìŠ¤ìº”
def directory(url,dirlist):
     with open(dirlist, 'rt', encoding='utf-8') as file:
        line = None  # ë³€ìˆ˜ lineì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
        while line != '':
            try:
                line = file.readline()
                URI = url + line.strip('\n')  # íŒŒì¼ì—ì„œ ì½ì–´ì˜¨ ë¬¸ìì—´ì—ì„œ \n ì‚­ì œí•˜ì—¬ ì €ì¥
                response = requests.get(URI, allow_redirects=False, verify=False)  # ë¦¬ìŠ¤íŒìŠ¤ ì €ì¥
                size = len(response.content)
                time.sleep(0.1)
                print(URI, response.status_code, size)
            except Exception:
                print('Connection Error :',url)
                pass

url = urlcheck(args.url)
print('target:{}'.format(url))

if args.option is 'd':
    dirlist = args.dir
    if dirlist is 'None':
        print('you need use option : --dir [url.txt]')
    else :
        directory(url,dirlist)

# url ì¶”ì¶œ
elif args.option is 'i':

    context = ssl._create_unverified_context()

    target = url
    html = urlopen(target, context=context)
    bs = BeautifulSoup(html, 'html.parser')

    internalLinks = []
    for link in bs.find_all('a', href=re.compile('^(/|.*' + target + ')')):
        if link.attrs['href'] is not None:
            if link.attrs['href'] not in internalLinks:
                if (link.attrs['href'].startswith('/')):
                    internalLinks.append(target + link.attrs['href'])
                else:
                    internalLinks.append(link.attrs['href'])
    i = '\n'.join(internalLinks)
    print(i)

# ì„œë¸Œë„ë©”ì¸ ìŠ¤ìº”
elif args.option is 's':
    parse = urlparse(args.url)
    url = parse.netloc
    response = requests.get('https://api.hackertarget.com/hostsearch/?q={}'.format(url))
    print(response.text)
else:
    print('usage: scanner_1.py [-h] option url')
</code></pre></div></div>

:ET